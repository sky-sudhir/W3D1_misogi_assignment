{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Prompt Engineering Pipeline Demo\n",
    "This notebook provides an interactive demonstration of the Prompt Engineering Pipeline, allowing you to:\n",
    "1. Create and manage tasks\n",
    "2. Run the pipeline with different configurations\n",
    "3. Visualize reasoning paths and results\n",
    "4. Optimize prompts interactively\n",
    "5. Analyze performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure required directories exist\n",
    "for directory in [\"logs\", \"prompts\", \"tasks\", \"evaluation\"]:\n",
    "    Path(directory).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Import pipeline components\n",
    "from src.main import Pipeline\n",
    "from src.task_manager import TaskManager\n",
    "from src.models import Task, TaskType\n",
    "from src.llm import LLMClient\n",
    "from src.reasoning_engine import ReasoningEngine\n",
    "from src.prompt_optimizer import PromptOptimizer\n",
    "from src.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "task_manager = TaskManager()\n",
    "llm_client = LLMClient()\n",
    "reasoning_engine = ReasoningEngine(llm_client=llm_client, num_paths=3)\n",
    "prompt_optimizer = PromptOptimizer()\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = Pipeline(\n",
    "    reasoning_engine=reasoning_engine,\n",
    "    prompt_optimizer=prompt_optimizer,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_interactive():\n",
    "    \"\"\"Create a new task interactively.\"\"\"\n",
    "    print(\"=== Create New Task ===\")\n",
    "    \n",
    "    # Get task type\n",
    "    print(\"\\nAvailable task types:\")\n",
    "    for i, task_type in enumerate(TaskType, 1):\n",
    "        print(f\"{i}. {task_type.value}\")\n",
    "    \n",
    "    type_idx = int(input(\"\\nSelect task type (number): \")) - 1\n",
    "    task_type = list(TaskType)[type_idx]\n",
    "    \n",
    "    # Get task details\n",
    "    description = input(\"\\nEnter task description: \")\n",
    "    expected_answer = input(\"Enter expected answer: \")\n",
    "    \n",
    "    # Create the task\n",
    "    task = task_manager.create_task(\n",
    "        task_type=task_type,\n",
    "        description=description,\n",
    "        expected_answer=expected_answer\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Created task: {task.id}\")\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tasks(show_all=False):\n",
    "    \"\"\"List all tasks with optional filtering.\"\"\"\n",
    "    tasks = task_manager.get_all_tasks()\n",
    "    \n",
    "    if not tasks:\n",
    "        print(\"No tasks found.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(tasks)} tasks:\")\n",
    "    \n",
    "    tasks_data = []\n",
    "    for i, task in enumerate(tasks, 1):\n",
    "        task_info = {\n",
    "            \"#\": i,\n",
    "            \"ID\": task.id,\n",
    "            \"Type\": task.type.value if isinstance(task.type, TaskType) else task.type,\n",
    "            \"Description\": (task.description[:50] + '...') if len(task.description) > 50 else task.description\n",
    "        }\n",
    "        tasks_data.append(task_info)\n",
    "    \n",
    "    # Display as a nice table\n",
    "    df = pd.DataFrame(tasks_data).set_index('#')\n",
    "    display(df)\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_interactive():\n",
    "    \"\"\"Run the pipeline interactively.\"\"\"\n",
    "    print(\"=== Run Pipeline ===\")\n",
    "    \n",
    "    # Let user select a task\n",
    "    tasks = list_tasks()\n",
    "    if not tasks:\n",
    "        print(\"No tasks available. Please create a task first.\")\n",
    "        return\n",
    "    \n",
    "    task_idx = int(input(\"\\nSelect task number to run: \")) - 1\n",
    "    task = tasks[task_idx]\n",
    "    \n",
    "    # Configure pipeline\n",
    "    num_paths = int(input(\"Number of reasoning paths (default 3): \") or \"3\")\n",
    "    pipeline.reasoning_engine = ReasoningEngine(\n",
    "        llm_client=llm_client,\n",
    "        num_paths=num_paths\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRunning pipeline...\")\n",
    "    result = pipeline.run_full_pipeline(task.dict())\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n=== Results ===\")\n",
    "    pprint(result)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        eval_result = result[\"evaluation\"]\n",
    "        print(f\"\\n✅ Task completed successfully\")\n",
    "        print(f\"Is correct: {eval_result['is_correct']}\")\n",
    "        print(f\"Confidence: {eval_result['confidence']:.2f}\")\n",
    "        print(f\"Prompt version: {result['prompt_version']}\")\n",
    "    else:\n",
    "        print(\"\\n❌ Task failed\")\n",
    "        print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
", "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_menu():\n",
    "    \"\"\"Display the interactive menu.\"\"\"\n",
    "    while True:\n",
    "        print(\"\\n=== Prompt Engineering Pipeline ===\")\n",
    "        print(\"1. List tasks\")\n",
    "        print(\"2. Create new task\")\n",
    "        print(\"3. Run pipeline\")\n",
    "        print(\"4. View prompt versions\")\n",
    "        print(\"5. View evaluation metrics\")\n",
    "        print(\"6. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-6): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            list_tasks()\n",
    "        elif choice == \"2\":\n",
    "            create_task_interactive()\n",
    "        elif choice == \"3\":\n",
    "            run_pipeline_interactive()\n",
    "        elif choice == \"4\":\n",
    "            view_prompt_versions()\n",
    "        elif choice == \"5\":\n",
    "            view_metrics()\n",
    "        elif choice == \"6\":\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nInvalid choice. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the interactive demo\n",
    "# show_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a math task\n",
    "math_task = task_manager.create_task(\n",
    "    task_type=TaskType.MATH,\n",
    "    description=\"What is the sum of the first 10 prime numbers?\",\n",
    "    expected_answer=\"129\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the pipeline\n",
    "result = pipeline.run_full_pipeline(math_task.dict())\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: View all tasks\n",
    "list_tasks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
